# Entrega Proyecto Parte 1 Sistemas Distribuidos ðŸš—ðŸ“Š

## Importante 
Actualmente te encuentras en nuestra rama main en donde se armÃ³ el cÃ³digo principal.
Sin embargo, en honor a los distintos tipos de cachÃ© implementados con sus respectivos distintos generadores de trÃ¡fico, se crearon 4 ramas en total: 

rama_cacheP1: Sistema de remociÃ³n del cache -> LRU - 50mb
              DistribuciÃ³n generador de trÃ¡fico -> Poisson
              
rama_cacheP2: Sistema de remociÃ³n del cache -> LFU - 100mb
              DistribuciÃ³n generador de trÃ¡fico -> Poisson
              
rama_cacheZ1: Sistema de remociÃ³n del cache -> LRU - 50mb
              DistribuciÃ³n generador de trÃ¡fico -> Zipf
              
rama_cacheP1: Sistema de remociÃ³n del cache -> LFU - 100mb
              DistribuciÃ³n generador de trÃ¡fico -> Zipf

Cada una de estas ramas tiene exactamente la misma estructura, sÃ³lo se hicieron los cambios correpondientes en docker-compose.yml para el tipo de cachÃ© y tamaÃ±o y los cambios correspondientes en gdt.py para el tipo de distribuciÃ³n seguida por el generador de trÃ¡fico.

## Arquitectura General

El sistema consta de los siguientes componentes:

- **Scraper**: Obtiene eventos de trÃ¡fico (como accidentes, atascos, etc.) desde la API pÃºblica del mapa en vivo de Waze. Guarda los eventos Ãºnicos en MongoDB.
- **Base de Datos (MongoDB)**: Almacena de manera persistente los eventos capturados por el Scraper.
- **CachÃ© (Flask + Redis)**: Expone una API REST para consultar eventos por UUID y guarda resultados temporalmente para acelerar accesos repetidos.
- **Generador de TrÃ¡fico**: Simula consultas concurrentes al sistema usando distribuciones probabilÃ­sticas (por ejemplo, Poisson), evaluando el comportamiento de la cachÃ© y la base de datos.
- **Filtrado y Procesamiento (Apache Pig)**: Procesa los datos recolectados, aplicando transformaciones, limpieza y agregaciones, todo ejecutado dentro de un contenedor Docker que simula un entorno Hadoop local.
- **Docker Compose**: Orquesta el despliegue de todos los servicios en contenedores separados, facilitando su ejecuciÃ³n conjunta.

---

##  Requisitos

- Docker
- Docker Compose
- Git

## ðŸš€ EjecuciÃ³n del sistema

1. Clona el repositorio

```bash
# Clona el repositorio
git clone https://github.com/tu-usuario/ProyectoSD.git.
cd ProyectoSD

# Levanta los servicios con Docker
sudo docker-compose up --build

# Si se quiere ejecutar otra vez sin haber hecho cambios en el codigo
sudo docker-compose up


```

## Sobre la ejecuciÃ³n del procesamiento y filtrado de datos 

```bash

docker-compose build pig  # esto es para crear la imagen de pig
docker-compose up -d pig # iniciar el contenedor
docker exec -it pig bash # acceder al contenedor

#ejecuciÃ³n de los sprits
pig -x local homogeneizacion.pig 
pig -x local procesamiento.pig

```

El scraper se ejecutarÃ¡ y guardarÃ¡ los datos en formato JSON en el contenedor como volumen para que se mantengan guardados.
Se levvantarÃ¡n todos los contenedores y se realizarÃ¡n consultas en seguida al sistema.

## ðŸ“‚ Estructura del proyecto
```bash
.
â”œâ”€â”€ scraper/              # Scraper de eventos desde Waze
â”‚   â”œâ”€â”€ scraper.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ cache/                # Servidor Flask + Redis como cachÃ©
â”‚   â”œâ”€â”€ cache.py
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ generador_trafico/    # Simula trÃ¡fico usando distintas distribuciones
â”‚   â”œâ”€â”€ generador.py
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-pig/           # MÃ³dulo de filtrado y anÃ¡lisis con Apache Pig
â”‚   â”œâ”€â”€ Dockerfile        # Imagen personalizada de Apache Pig
â”‚   â”œâ”€â”€ data.tsv          # Datos crudos de eventos
â”‚   â”œâ”€â”€ Filtrado.pig
â”‚   â””â”€â”€ procesamiento.pig
â”‚
â”œâ”€â”€ docker-compose.yml    # OrquestaciÃ³n de todos los contenedores
â””â”€â”€ README.md             # Este archivo

```
## ðŸ›  TecnologÃ­as utilizadas

Python (scraping, servidor REST, generaciÃ³n de trÃ¡fico)

MongoDB (almacenamiento)

Redis (cachÃ© con polÃ­ticas LRU/LFU)

Flask (API REST)

Apache Pig (procesamiento tipo MapReduce)

Hadoop (modo local para Pig)

Docker & Docker Compose (contenedorizaciÃ³n y despliegue)

## Estado actual

âœ” Scraper funcional conectado a MongoDB
âœ” API REST con cachÃ© y TTL configurable
âœ” Generador de carga que simula trÃ¡fico realista
âœ” Sistema completamente containerizado
âœ” DocumentaciÃ³n tÃ©cnica en LaTeX incluida

## ðŸ“Œ Notas

- El scraping solo obtiene datos visibles en el mapa en vivo al momento de ejecuciÃ³n.
- La base de datos aÃºn no esta subida de forma remota a Mongo, sÃ³lo la manejamos de forma local.

## Autoras 

Isidora GonzÃ¡lez
Natalia Ortega


